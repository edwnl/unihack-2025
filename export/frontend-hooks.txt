# frontend-hooks Files

### frontend\hooks\useAzureSpeechRecognition.ts

```ts
"use client";

import { useState, useRef, useEffect } from "react";
import * as SpeechSDK from "microsoft-cognitiveservices-speech-sdk";

/**
 * Return type for our Azure speech hook.
 */
export type UseAzureSpeechRecognitionReturn = {
  isListening: boolean;
  recognizedText: string;
  startListening: () => void;
  stopListening: () => void;
  clearRecognizedText: () => void;
};

/**
 * Custom hook that uses Microsoft Cognitive Services Speech SDK for real-time recognition.
 * `onTranscriptReceived` is a callback that runs each time new text is recognized.
 */
export function useAzureSpeechRecognition(
  onTranscriptReceived?: (transcript: string) => void,
): UseAzureSpeechRecognitionReturn {
  const [isListening, setIsListening] = useState(false);
  const [recognizedText, setRecognizedText] = useState("");
  const recognizerRef = useRef<SpeechSDK.SpeechRecognizer | null>(null);

  // Read subscription details from environment variables.
  // Make sure these are set in .env.local with the NEXT_PUBLIC_ prefix.
  const subscriptionKey = process.env.NEXT_PUBLIC_AZURE_SPEECH_KEY || "";
  const serviceRegion = "australiaeast";

  /**
   * Start listening continuously using Azure Speech SDK.
   */
  const startListening = () => {
    console.log("[AzureSpeech] startListening called");
    if (isListening) {
      console.log("[AzureSpeech] Already listening, skipping start.");
      return;
    }

    if (!subscriptionKey || !serviceRegion) {
      console.error(
        "[AzureSpeech] Subscription key or service region is missing.",
      );
      return;
    }

    // Configure the Azure Speech SDK
    console.log("[AzureSpeech] Creating speech configuration...");
    const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(
      subscriptionKey,
      serviceRegion,
    );
    speechConfig.speechRecognitionLanguage = "en-US"; // Adjust language as needed
    speechConfig.endpointId = "de96dcbf-e1b3-4373-bdad-5a7d4ef91f2a";
    console.log("[AzureSpeech] Speech configuration created:", speechConfig);

    console.log(
      "[AzureSpeech] Creating audio configuration from default microphone...",
    );
    const audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
    console.log("[AzureSpeech] Audio configuration created:", audioConfig);

    const recognizer = new SpeechSDK.SpeechRecognizer(
      speechConfig,
      audioConfig,
    );
    recognizerRef.current = recognizer;
    console.log("[AzureSpeech] SpeechRecognizer instance created.");

    // This fires when the service has recognized (finalized) some speech
    recognizer.recognized = (_sender, event) => {
      console.log(
        "[AzureSpeech] Recognized event fired with result:",
        event.result,
      );
      if (event.result.reason === SpeechSDK.ResultReason.RecognizedSpeech) {
        console.log("[AzureSpeech] Recognized speech:", event.result.text);
        setRecognizedText(event.result.text);
        if (onTranscriptReceived) {
          onTranscriptReceived(event.result.text);
        }
      } else {
        console.warn(
          "[AzureSpeech] Recognized event reason not RecognizedSpeech:",
          event.result.reason,
        );
      }
    };

    // Start continuous recognition
    console.log("[AzureSpeech] Starting continuous recognition...");
    recognizer.startContinuousRecognitionAsync(
      () => {
        console.log(
          "[AzureSpeech] Continuous recognition started successfully.",
        );
        setIsListening(true);
      },
      (error) => {
        console.error(
          "[AzureSpeech] Error starting continuous recognition:",
          error,
        );
        setIsListening(false);
      },
    );
  };

  /**
   * Stop listening and release the recognizer.
   */
  const stopListening = () => {
    console.log("[AzureSpeech] stopListening called");
    if (!recognizerRef.current) {
      console.log("[AzureSpeech] No active recognizer to stop.");
      return;
    }

    recognizerRef.current.stopContinuousRecognitionAsync(
      () => {
        console.log(
          "[AzureSpeech] Continuous recognition stopped successfully.",
        );
        setIsListening(false);
        if (recognizerRef.current) {
          recognizerRef.current.close();
          console.log("[AzureSpeech] Recognizer closed.");
        }
        recognizerRef.current = null;
      },
      (error) => {
        console.error(
          "[AzureSpeech] Error stopping continuous recognition:",
          error,
        );
        setIsListening(false);
      },
    );
  };

  /**
   * Clears out recognized text in the UI.
   */
  const clearRecognizedText = () => {
    console.log("[AzureSpeech] clearRecognizedText called.");
    setRecognizedText("");
  };

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      console.log("[AzureSpeech] Cleanup effect triggered.");
      if (recognizerRef.current) {
        recognizerRef.current.close();
        console.log("[AzureSpeech] Recognizer closed during cleanup.");
        recognizerRef.current = null;
      }
    };
  }, []);

  return {
    isListening,
    recognizedText,
    startListening,
    stopListening,
    clearRecognizedText,
  };
}

```


### frontend\hooks\useAzureSpeechSynthesis.ts

```ts
// frontend/hooks/useAzureSpeechSynthesis.ts
"use client";

import { useState, useRef, useCallback } from "react";
import * as SpeechSDK from "microsoft-cognitiveservices-speech-sdk";

/**
 * Return type for our Azure speech synthesis hook.
 */
export type UseAzureSpeechSynthesisReturn = {
  isSpeaking: boolean;
  speak: (text: string) => void;
  stopSpeaking: () => void;
};

/**
 * Custom hook that uses Microsoft Cognitive Services Speech SDK for text-to-speech.
 * Implements a proper queue system to ensure announcements play sequentially.
 */
export function useAzureSpeechSynthesis(): UseAzureSpeechSynthesisReturn {
  const [isSpeaking, setIsSpeaking] = useState(false);
  const synthesizerRef = useRef<SpeechSDK.SpeechSynthesizer | null>(null);
  const queueRef = useRef<string[]>([]);
  const isProcessingRef = useRef<boolean>(false);

  // Read subscription details from environment variables
  const subscriptionKey = process.env.NEXT_PUBLIC_AZURE_SPEECH_KEY || "";
  const serviceRegion = process.env.NEXT_PUBLIC_AZURE_SPEECH_REGION || "australiaeast";

  // Function to speak a single text item
  const speakText = useCallback(async (text: string): Promise<void> => {
    if (!text || !subscriptionKey || !serviceRegion) return Promise.resolve();

    console.log("[AzureSpeech] Speaking:", text);
    setIsSpeaking(true);

    try {
      // Configure the Azure Speech SDK
      const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(subscriptionKey, serviceRegion);
      speechConfig.speechSynthesisLanguage = "en-US";
      speechConfig.speechSynthesisVoiceName = "en-US-JennyNeural"; // Use a modern, clear voice

      // Create the synthesizer
      const audioConfig = SpeechSDK.AudioConfig.fromDefaultSpeakerOutput();
      const synthesizer = new SpeechSDK.SpeechSynthesizer(speechConfig, audioConfig);
      synthesizerRef.current = synthesizer;

      return new Promise<void>((resolve, reject) => {
        synthesizer.speakTextAsync(
          text,
          (result) => {
            if (result.reason === SpeechSDK.ResultReason.SynthesizingAudioCompleted) {
              console.log("[AzureSpeech] Speech synthesis completed successfully.");
              synthesizer.close();
              synthesizerRef.current = null;
              resolve();
            } else {
              const cancellationDetails = SpeechSDK.CancellationDetails.fromResult(result);
              console.error("[AzureSpeech] Speech synthesis canceled:", cancellationDetails.errorDetails);
              synthesizer.close();
              synthesizerRef.current = null;
              reject(new Error(cancellationDetails.errorDetails));
            }
          },
          (error) => {
            console.error("[AzureSpeech] Speech synthesis error:", error);
            synthesizer.close();
            synthesizerRef.current = null;
            reject(error);
          }
        );
      });
    } catch (error) {
      console.error("[AzureSpeech] Error during speech synthesis:", error);
      return Promise.reject(error);
    }
  }, [subscriptionKey, serviceRegion]);

  // Process the next item in the queue
  const processQueue = useCallback(async () => {
    // If already processing or no items in queue, exit
    if (isProcessingRef.current || queueRef.current.length === 0) {
      if (queueRef.current.length === 0) {
        setIsSpeaking(false);
      }
      return;
    }

    // Mark as processing
    isProcessingRef.current = true;
    
    try {
      // Get the next text to speak (but don't remove it yet)
      const nextText = queueRef.current[0];
      
      // Speak the text
      await speakText(nextText);
      
      // Remove the text from the queue only after it's been spoken
      queueRef.current.shift();
      
    } catch (error) {
      console.error("[AzureSpeech] Error processing queue item:", error);
    } finally {
      // Mark as not processing anymore
      isProcessingRef.current = false;
      
      // Small delay before processing next item to prevent overlapping
      setTimeout(() => {
        // Process the next item if any
        if (queueRef.current.length > 0) {
          processQueue();
        } else {
          setIsSpeaking(false);
        }
      }, 500); // Added a slightly longer delay between phrases
    }
  }, [speakText]);

  /**
   * Add text to the speech queue and start processing if not already processing
   */
  const speak = useCallback((text: string): void => {
    if (!text) {
      console.log("[AzureSpeech] No text provided, skipping speech synthesis.");
      return;
    }

    if (!subscriptionKey || !serviceRegion) {
      console.error("[AzureSpeech] Subscription key or service region is missing.");
      return;
    }

    console.log("[AzureSpeech] Adding to queue:", text);

    // Add the text to the queue
    queueRef.current.push(text);
    setIsSpeaking(true);
    
    // If not already processing, start processing the queue
    if (!isProcessingRef.current) {
      processQueue();
    }
  }, [processQueue, subscriptionKey, serviceRegion]);

  const stopSpeaking = useCallback(() => {
    if (synthesizerRef.current) {
      synthesizerRef.current.close();
      synthesizerRef.current = null;
    }
    
    // Clear the queue
    queueRef.current = [];
    isProcessingRef.current = false;
    setIsSpeaking(false);
  }, []);

  return {
    isSpeaking,
    speak,
    stopSpeaking,
  };
}
```
